{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification: One vs All\n",
    "### A study of Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to do classification where our result can be of different Discrete types.\n",
    "That is y can take ***Multiple Discrete values***, and not only two values 0 and 1, as before. \n",
    "\n",
    "In this case we can use Logistic Regression to classify the data by using the ***one vs all approach***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Iris dataset which is quite a famous dataset. And we will classify the iris flowers into their three species ***Setosa, Versicolour and Virginica*** based on their ***petal and sepal, length and width***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] are the feature names of the iris dataset\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "print(type(data))\n",
    "print(iris.feature_names,'are the feature names of the iris dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   s_l  s_w  p_l  p_w\n",
      "0  5.1  3.5  1.4  0.2\n",
      "1  4.9  3.0  1.4  0.2\n",
      "2  4.7  3.2  1.3  0.2\n",
      "3  4.6  3.1  1.5  0.2\n",
      "4  5.0  3.6  1.4  0.2\n",
      "     s_l  s_w  p_l  p_w\n",
      "145  6.7  3.0  5.2  2.3\n",
      "146  6.3  2.5  5.0  1.9\n",
      "147  6.5  3.0  5.2  2.0\n",
      "148  6.2  3.4  5.4  2.3\n",
      "149  5.9  3.0  5.1  1.8\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(data, columns=['s_l','s_w','p_l','p_w']) \n",
    "print(data.head(5))\n",
    "print(data.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these above are the features which classify an iris flower into three species. Now we will load the iris.target in vector Y, which will have basically three values repeatedely, each corresponding to a species:\n",
    "    \n",
    "    y = 0 --> Setosa\n",
    "    y = 1 --> Versicolour\n",
    "    y = 2 --> Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "dtype: int64\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Y = pd.Series(iris.target)\n",
    "print(Y.head(5))\n",
    "print(Y.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one vs all technique, we run the logistic regression taking one feature and considering rest of the features as another feature, which reduces it to a binary feature Logistic Regression.\n",
    "\n",
    "So let's do it for Setosa vs. rest first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Descent(x, y, l_t, itr):\n",
    "    #assuming x is (m,n) shaped and y is (m,) or (m,1) shaped\n",
    "    m = x.shape[0]\n",
    "    y = y.reshape(m,1)\n",
    "    x = np.concatenate((np.ones((m,1)),x), axis=1) \n",
    "    n = x.shape[1]\n",
    "    theta = np.random.random(size=(1,n))*0.01\n",
    "    \n",
    "    for i in range(itr):\n",
    "        h_xi = (1/(1 + np.exp(-np.dot(x,np.transpose(theta)))))\n",
    "        theta = theta - (l_t/m) * np.dot(np.transpose(h_xi- y) , x)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "x1 = data.values\n",
    "y1 = np.concatenate((Y[0:50,].values.reshape(50,1), (np.ones((100,1))*3)), axis=0)\n",
    "print( y1[:5,:], y1[145:150,:], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in array y, indices 0 to 49, have value=0, indicating setosa. And indices 50 to 149 have  value=3, indicating the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.01939523 6.74629184 2.7235824  6.09503949 2.16395269]]\n"
     ]
    }
   ],
   "source": [
    "theta1 = Gradient_Descent(x1, y1, 0.01, 100)\n",
    "print(theta1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
